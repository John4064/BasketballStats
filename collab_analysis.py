# -*- coding: utf-8 -*-
"""analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14yaL8EEqAvJeeTQRK0Q2g-OcJVTmUF6B
Here in the collab is all the analysis and data models expanded and explained more in detail. Attached to the submission is the python files for the GUI as well as some screenshots. I am analyzing a College Basketball data set from 2018. My goal is to determine and show the general direction college basketball is currently heading. Specifically in Offense vs Defense and shot selection of 2p to 3p.
"""

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import euclidean_distances
def classification_confint(acc, n):
    '''
    Compute the 95% confidence interval for a classification problem.
      acc -- classification accuracy
      n   -- number of observations used to compute the accuracy
    Returns a tuple (lb,ub)
    '''
    import math
    interval = 1.96*math.sqrt(acc*(1-acc)/n)
    lb = max(0, acc - interval)
    ub = min(1.0, acc + interval)
    return (lb,ub)

"""This is just cleaning the data to remove the columns with little use/empty. We save these specific rows with information in the column however to use for future as they are the top teams in the country."""

df = pd.read_csv("cbb18.csv")
#This is the post season only teams
postdf= df.dropna()
df= df.drop(["POSTSEASON"],axis=1)
df= df.drop(["SEED"],axis=1)
#group by conference
#percentage
df.describe()

"""Our Dataset has 351 Rows and 22 Columns It includes the top 351 college basketball teams for year 2018. (URI made it far in the tournament) and that is why I picked this particular year

Above we can see the the max for wins was 36 wins on the year. the average field goal percentage was 50.8% and average turovers was 18.44. Those are surprisng numbers as I expected it to be sub 50% and around 14-15 turnovers per game similar/slightly less to the nba. To see some of the example data in clear text form below we can
"""

df.value_counts()

"""Below we can see two histograms for Wins and  Adjusted offensive efficiency. We see that majority of the wins are concentrated between 10-25 with barely any teams cracking above 26 and below 9. With ADJ Offensive efficiency we see most teams are scoring between 95 and 115 points per game with the adjusted score. That is a very broad range but what surprised me was the amount of teams that were below 95. I figured there would not be a single team below 95 as that is extremely low for a game let alone a season."""

df.hist(['W'])

df.hist(['ADJOE'])

"""A scatter plot to analyze effective field goal percentage vs win percentage. As expected higher field goal percentage tends to have higher win percentage. We choose the top 3 college basketball conferences for this to clean up the scatter plot and see how each conference faired vs each other as well.

"""

grouped = df.groupby(df.CONF)
ACC_df = grouped.get_group("ACC")
B10_df = grouped.get_group("B10")
BE_df = grouped.get_group("BE")
plt.plot((ACC_df['W']/ACC_df['G']),ACC_df['EFG_O'],'bo')
plt.plot((B10_df['W']/B10_df['G']),B10_df['EFG_O'],'go')
plt.plot((BE_df['W']/BE_df['G']),BE_df['EFG_O'],'ro')
plt.axis([0,1,42,60])
plt.xlabel('Win Percentage')
plt.ylabel('Effective Field Goal Percentage')
plt.legend(['ACC','Big 10','Big East'])

"""Looking at Offensive 3p% vs defense again 3p% using the big three conferences. No real trend found here"""

grouped = df.groupby(df.CONF)
ACC_df = grouped.get_group("ACC")
B10_df = grouped.get_group("B10")
BE_df = grouped.get_group("BE")
plt.plot((ACC_df['3P_D']),ACC_df['3P_O'],'bo')
plt.plot((B10_df['3P_D']),B10_df['3P_O'],'go')
plt.plot((BE_df['3P_D']),BE_df['3P_O'],'ro')
plt.axis([29,40,29,42])
plt.xlabel('3P%')
plt.ylabel('Opponent 3p%')
plt.legend(['ACC','Big 10','Big East'])

"""Another look at offense vs defense with the teams fg% vs their opponnents fg%. This is a breakdown to see how many teams are offense focussed vs defense focus. WE can see that there is a slight minor trend this year with more teams being offense focused than defense focused."""

df.loc[:,'EFG_O'].plot.kde(ind=[ 40, 44,48,52,56,60,62])
df.loc[:,'EFG_D'].plot.kde(ind=[ 40, 44,48,52,56,60,62])
plt.xlabel('FG%')
plt.legend(['Teams FG%','Opponents FG%'])

"""For our Machine Learning model we are going to do two kinds of model a KNN classification model and an ANN model.
First we will be doing a single layer ANN model on how the stats effect win totals.
 We will be doing a grid search to find the optimal hidden layer sizes and activation functions
"""

X = df
# We drop TEAM and CONF because we dont want the strings
#G and W are dropped as Wins is our target and number Games should not be in consideration
#As they play near identical amount.
X=X.drop(['TEAM'],axis=1)
X=X.drop(['CONF'],axis=1)
X=X.drop(['G'],axis=1)
X=X.drop(['W'],axis=1)
y = df['W']
#Training was taking forever so decreased sample size
X=X[:125]
y=y[:125]
model = MLPClassifier(max_iter=10000)
#We calculate 10-20 for the variable parameter of hidden layer sizes
param_grid = {'hidden_layer_sizes': [ (10,), (11,), (12,), (13,), 
                                     (14,), (15,), (16,), (17,), 
                                     (18,), (19,), (20,)],
               'activation': ['relu','logistic']}
grid = GridSearchCV(model, param_grid, cv=3)
grid.fit(X, y)
print("Grid Search: best parameters: {}".format(grid.best_params_))

"""We are going to evlaulate the best parameters for the mode to determine the accuracy of it. Wefound the the Activation type logisitic and 14, was the best combination for the single layer mlp. We will be using the classification confint for it."""

best_model = grid.best_estimator_
predict_y = best_model.predict(X)
acc = accuracy_score(y, predict_y)
lb,ub = classification_confint(acc,X.shape[0])
print("Accuracy: {:3.2f} ({:3.2f},{:3.2f})".format(acc,lb,ub))

"""So we ended with a 98% Accuracy score for the single layer mlp with parameters of 14, for the hidden layer size and relu for activation type.
Below is the KNN
"""

#We can use X and Y from above for this
modelK = KNeighborsClassifier()

param_gridK = {'n_neighbors': list(range(1,52))}
gridK = GridSearchCV(modelK, param_gridK, cv=5)
gridK.fit(X, y)
print("Grid Search: best parameters: {}".format(gridK.best_params_))

"""We see that the best amount of neighbors is 32."""

model2 = KNeighborsClassifier(n_neighbors=32)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=2)
model2.fit(X_train,y_train)
y_test_model = model2.predict(X_test)

best_model = gridK.best_estimator_
predict_y2 = best_model.predict(X)
acc2 = accuracy_score(y, predict_y2)
lb2,ub2 = classification_confint(acc2,X.shape[0])

print("KNN")
print("Accuracy: {:3.2f} ({:3.2f},{:3.2f})".format(acc2,lb2,ub2))

"""Which comes out to an accuracy of 23% with these parameters. Compared to a 98% using the single layer MLP. That is a massive difference of 75% accuracy of prediciting College basketball wins at the end of a season. Something to consider for future analysis is how these models hold up with other college basketball seasons.

Down Below are the final two data models Used in the GUI
"""

fig = plt.figure()
ax = fig.add_subplot(111)
grouped = df.groupby(df.CONF)
ACC_df = grouped.get_group("ACC")
ACC_stat = ACC_df['2P_O']/ACC_df['2P_D']

B10_df = grouped.get_group("B10")
B10_stat = B10_df['2P_O']/B10_df['2P_D']
BE_df = grouped.get_group("BE")
BE_stat = BE_df['2P_O']/BE_df['2P_D']
P12_df = grouped.get_group("P12")
P12_stat = P12_df['2P_O']/P12_df['2P_D']
ax.set_ylabel("2P Off VS Def Shot %")
ax.bar("ACC", ACC_stat, color = 'b', width = 0.8)
ax.bar("Big 10", B10_stat, color = 'g', width = 0.8)
ax.bar("Big East", BE_stat, color = 'r', width = 0.8)
ax.bar("Pac 12", P12_stat, color = 'y', width = 0.8)
ax.set_ylim(ymin=1)
plt.show()

fig1 = plt.figure()
ax1 = fig1.add_axes([0,0,1,1])
grouped = df.groupby(df.CONF)
ACC_df = grouped.get_group("ACC")
B10_df = grouped.get_group("B10")
BE_df = grouped.get_group("BE")
P12_df = grouped.get_group("P12")
ax1.set_ylabel("Wins")
ax1.bar("ACC", ACC_df['W'], color = 'b', width = 0.8)
ax1.bar("Big 10", B10_df['W'], color = 'g', width = 0.8)
ax1.bar("Big East", BE_df['W'], color = 'r', width = 0.8)
ax1.bar("Pac 12", P12_df['W'], color = 'y', width = 0.8)
ax1.set_ylim(ymin=20)
plt.show()